{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Judy-Choi/Tensorflow_Certificate/blob/main/17_satellitehurricane_ans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.10.0"
      ],
      "metadata": {
        "id": "KlNMXyj3jD2t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31898d80-d026-4b17-bc1f-01525f09d444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.12.0\n",
            "Uninstalling tensorflow-2.12.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow-2.12.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.12.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.10.0\n",
            "  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.8.0)\n",
            "Collecting keras<2.11,>=2.10.0 (from tensorflow==2.10.0)\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.10.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (23.1)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.10.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.16.0)\n",
            "Collecting tensorboard<2.11,>=2.10 (from tensorflow==2.10.0)\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.32.0)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow==2.10.0)\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n",
            "Installing collected packages: keras, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.2\n",
            "    Uninstalling tensorboard-2.12.2:\n",
            "      Successfully uninstalled tensorboard-2.12.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.10.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmxrhftEhuTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bec84b-c175-4eeb-f083-74c3c6583184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10000 files belonging to 2 classes.\n",
            "Found 2000 files belonging to 2 classes.\n",
            "Epoch 1/50\n",
            "155/157 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.7376\n",
            "Epoch 1: val_loss improved from inf to 0.35034, saving model to tmp_checkpoint.ckpt\n",
            "157/157 [==============================] - 3s 15ms/step - loss: 0.5147 - acc: 0.7386 - val_loss: 0.3503 - val_acc: 0.8540\n",
            "Epoch 2/50\n",
            "154/157 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.8948\n",
            "Epoch 2: val_loss improved from 0.35034 to 0.25283, saving model to tmp_checkpoint.ckpt\n",
            "157/157 [==============================] - 2s 13ms/step - loss: 0.2531 - acc: 0.8956 - val_loss: 0.2528 - val_acc: 0.9065\n",
            "Epoch 3/50\n",
            "156/157 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9337\n",
            "Epoch 3: val_loss improved from 0.25283 to 0.15778, saving model to tmp_checkpoint.ckpt\n",
            "157/157 [==============================] - 2s 13ms/step - loss: 0.1602 - acc: 0.9338 - val_loss: 0.1578 - val_acc: 0.9380\n",
            "Epoch 4/50\n",
            "152/157 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9453\n",
            "Epoch 4: val_loss improved from 0.15778 to 0.15318, saving model to tmp_checkpoint.ckpt\n",
            "157/157 [==============================] - 2s 13ms/step - loss: 0.1387 - acc: 0.9456 - val_loss: 0.1532 - val_acc: 0.9410\n",
            "Epoch 5/50\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.0989 - acc: 0.9615\n",
            "Epoch 5: val_loss improved from 0.15318 to 0.12194, saving model to tmp_checkpoint.ckpt\n",
            "157/157 [==============================] - 2s 13ms/step - loss: 0.0989 - acc: 0.9615 - val_loss: 0.1219 - val_acc: 0.9580\n",
            "Epoch 6/50\n",
            "152/157 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9590\n",
            "Epoch 6: val_loss did not improve from 0.12194\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.1023 - acc: 0.9592 - val_loss: 0.1833 - val_acc: 0.9420\n",
            "Epoch 7/50\n",
            "155/157 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9630\n",
            "Epoch 7: val_loss did not improve from 0.12194\n",
            "157/157 [==============================] - 2s 13ms/step - loss: 0.0933 - acc: 0.9631 - val_loss: 0.1239 - val_acc: 0.9550\n",
            "Epoch 8/50\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.0674 - acc: 0.9749\n",
            "Epoch 8: val_loss did not improve from 0.12194\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0674 - acc: 0.9749 - val_loss: 0.1423 - val_acc: 0.9510\n",
            "Epoch 9/50\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.0469 - acc: 0.9826\n",
            "Epoch 9: val_loss did not improve from 0.12194\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0469 - acc: 0.9826 - val_loss: 0.1515 - val_acc: 0.9455\n",
            "Epoch 10/50\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.0499 - acc: 0.9805\n",
            "Epoch 10: val_loss improved from 0.12194 to 0.10893, saving model to tmp_checkpoint.ckpt\n",
            "157/157 [==============================] - 2s 13ms/step - loss: 0.0499 - acc: 0.9805 - val_loss: 0.1089 - val_acc: 0.9645\n",
            "Epoch 11/50\n",
            "152/157 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9848\n",
            "Epoch 11: val_loss did not improve from 0.10893\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0386 - acc: 0.9851 - val_loss: 0.1220 - val_acc: 0.9685\n",
            "Epoch 12/50\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.0354 - acc: 0.9859\n",
            "Epoch 12: val_loss did not improve from 0.10893\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0354 - acc: 0.9859 - val_loss: 0.1578 - val_acc: 0.9645\n",
            "Epoch 13/50\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.0322 - acc: 0.9908\n",
            "Epoch 13: val_loss did not improve from 0.10893\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0322 - acc: 0.9908 - val_loss: 0.1499 - val_acc: 0.9675\n",
            "Epoch 14/50\n",
            "152/157 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9953\n",
            "Epoch 14: val_loss did not improve from 0.10893\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0153 - acc: 0.9953 - val_loss: 0.1322 - val_acc: 0.9665\n",
            "Epoch 15/50\n",
            "152/157 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9914\n",
            "Epoch 15: val_loss did not improve from 0.10893\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0231 - acc: 0.9915 - val_loss: 0.1709 - val_acc: 0.9630\n",
            "Epoch 16/50\n",
            "152/157 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9923\n",
            "Epoch 16: val_loss did not improve from 0.10893\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0223 - acc: 0.9924 - val_loss: 0.1561 - val_acc: 0.9690\n",
            "Epoch 17/50\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.0196 - acc: 0.9930\n",
            "Epoch 17: val_loss did not improve from 0.10893\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0196 - acc: 0.9930 - val_loss: 0.1884 - val_acc: 0.9680\n",
            "Epoch 18/50\n",
            "152/157 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9969\n",
            "Epoch 18: val_loss did not improve from 0.10893\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.1811 - val_acc: 0.9675\n",
            "Epoch 19/50\n",
            "152/157 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9943\n",
            "Epoch 19: val_loss did not improve from 0.10893\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0166 - acc: 0.9944 - val_loss: 0.2400 - val_acc: 0.9535\n",
            "Epoch 20/50\n",
            "153/157 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9955\n",
            "Epoch 20: val_loss did not improve from 0.10893\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0146 - acc: 0.9955 - val_loss: 0.1743 - val_acc: 0.9665\n"
          ]
        }
      ],
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly less than\n",
        "# your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# COMPUTER VISION WITH CNNs\n",
        "#\n",
        "# Create and train a classifier to classify images between two classes\n",
        "# (damage and no_damage) using the satellite-images-of-hurricane-damage dataset.\n",
        "# ======================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://ieee-dataport.org/open-access/detecting-damaged-buildings-post-hurricane-satellite-imagery-based-customized\n",
        "# The dataset consists of satellite images from Texas after Hurricane Harvey\n",
        "# divided into two groups (damage and no_damage).\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# We have already divided the data for training and validation.\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. preprocess()\n",
        "# 2. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "# 1. The input shape of your model must be (128,128,3), because the testing\n",
        "#    infrastructure expects inputs according to this specification. You must\n",
        "#    resize all the images in the dataset to this size while pre-processing\n",
        "#    the dataset.\n",
        "# 2. The last layer of your model must be a Dense layer with 1 neuron\n",
        "#    activated by sigmoid since this dataset has 2 classes.\n",
        "#\n",
        "# HINT: Your neural network must have a validation accuracy of approximately\n",
        "# 0.95 or above on the normalized validation dataset for top marks.\n",
        "\n",
        "import urllib\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "# This function downloads and extracts the dataset to the directory that\n",
        "# contains this file.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "# (unless you need to change https to http)\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/satellitehurricaneimages.zip'\n",
        "    urllib.request.urlretrieve(url, 'satellitehurricaneimages.zip')\n",
        "    with zipfile.ZipFile('satellitehurricaneimages.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "# This function normalizes the images.\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def preprocess(image, label):\n",
        "    # NORMALIZE YOUR IMAGES HERE (HINT: Rescale by 1/.255)\n",
        "    image /=255\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# This function loads the data, normalizes and resizes the images, splits it into\n",
        "# train and validation sets, defines the model, compiles it and finally\n",
        "# trains the model. The trained model is returned from this function.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION.\n",
        "def solution_model():\n",
        "    # Downloads and extracts the dataset to the directory that\n",
        "    # contains this file.\n",
        "    download_and_extract_data()\n",
        "\n",
        "    IMG_SIZE = 128\n",
        "    BATCH_SIZE = 64\n",
        "\n",
        "    # The following code reads the training and validation data from their\n",
        "    # respective directories, resizes them into the specified image size\n",
        "    # and splits them into batches. You must fill in the image_size\n",
        "    # argument for both training and validation data.\n",
        "    # HINT: Image size is a tuple\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory='train/',\n",
        "        label_mode='binary',\n",
        "\t    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE)\n",
        "\n",
        "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        " \t    directory='validation/',\n",
        "\t    label_mode='binary',\n",
        "\t    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "\t    batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Normalizes train and validation datasets using the\n",
        "    # preprocess() function.\n",
        "    # Also makes other calls, as evident from the code, to prepare them for\n",
        "    # training.\n",
        "    # Do not batch or resize the images in the dataset here since it's already\n",
        "    # been done previously.\n",
        "    train_ds = train_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n",
        "        tf.data.experimental.AUTOTUNE)\n",
        "    val_ds = val_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Code to define the model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # ADD LAYERS OF THE MODEL HERE\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Flatten(),\n",
        "        Dropout(0.25),\n",
        "        Dense(128, activation='relu'),\n",
        "        # If you don't adhere to the instructions in the following comments,\n",
        "        # tests will fail to grade your model:\n",
        "        # The input layer of your model must have an input shape of\n",
        "        # (128,128,3).\n",
        "        # Make sure your last layer has 1 neuron activated by sigmoid.\n",
        "        tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "    ])\n",
        "\n",
        "    # Code to compile and train the model\n",
        "    model.compile(\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['acc']\n",
        "    )\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    checkpoint_path = 'tmp_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                 save_weights_only=True,\n",
        "                 save_best_only=True,\n",
        "                 monitor='val_loss',\n",
        "                 verbose=1)\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=50,\n",
        "        callbacks=[checkpoint, early_stopping]\n",
        "    )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ]
    }
  ]
}